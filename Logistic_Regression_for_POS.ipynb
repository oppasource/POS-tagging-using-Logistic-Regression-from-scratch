{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tag using Logistic Regression\n",
    "\n",
    "## Loading word embeddings\n",
    "First we load the pretrained GloVe word embeddings trained on twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "# Create a directory 'pretrained_embeds/' in the same directory as this notebook\n",
    "# Download twitter embeddings from http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "# Unzip it and place file 'glove.twitter.27B.25d.txt' in 'pretrained_embeds/' directory.\n",
    "\n",
    "# We are doing it with 25 dimensional word embeddings, however we can try doing with more \n",
    "# dimensional embeddings available.\n",
    "\n",
    "# If glove embeds is not in word2vec form then first convert it then load it\n",
    "if os.path.isfile('pretrained_embeds/gensim_glove_vectors.txt'):\n",
    "    glove_model = KeyedVectors.load_word2vec_format(\"pretrained_embeds/gensim_glove_vectors.txt\", binary=False)\n",
    "else:\n",
    "    glove2word2vec(glove_input_file=\"pretrained_embeds/glove.twitter.27B.25d.txt\", word2vec_output_file=\"pretrained_embeds/gensim_glove_vectors.txt\")\n",
    "    glove_model = KeyedVectors.load_word2vec_format(\"pretrained_embeds/gensim_glove_vectors.txt\", binary=False)\n",
    "\n",
    "def get_embed(word):\n",
    "    # Case folding\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        return (glove_model.get_vector(word))\n",
    "    except:\n",
    "        return (glove_model.get_vector('unk'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset\n",
    "\n",
    "Loading data using nltk (we are using brown corpus) and splitting data in train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "tagged_sents = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "# Splitting train and test(80:20)\n",
    "train_len = int(len(tagged_sents) * 0.8)\n",
    "train_sents = tagged_sents[:train_len]\n",
    "test_sents = tagged_sents[train_len:]\n",
    "\n",
    "brown_tags_words = []\n",
    "train_tags = []\n",
    "train_words = []\n",
    "train_embeds = []\n",
    "\n",
    "test_tags = []\n",
    "test_words = []\n",
    "test_embeds = []\n",
    "\n",
    "# Create Train data\n",
    "for sent in train_sents:\n",
    "    brown_tags_words.extend([ (tag, word) for (word, tag) in sent ])\n",
    "\n",
    "# Seperate out tag and word sequences\n",
    "for (tag, word) in brown_tags_words:\n",
    "    train_tags.append(tag)\n",
    "    train_words.append(word)\n",
    "    train_embeds.append(get_embed(word))\n",
    "    \n",
    "brown_tags_words = []\n",
    "# Create Test data\n",
    "for sent in test_sents:\n",
    "    brown_tags_words.extend([ (tag, word) for (word, tag) in sent ])\n",
    "\n",
    "# Seperate out tag and word sequences\n",
    "for (tag, word) in brown_tags_words:\n",
    "    test_tags.append(tag)\n",
    "    test_words.append(word)\n",
    "    test_embeds.append(get_embed(word))\n",
    "\n",
    "# Adding bias at the end of each embedding\n",
    "train_embeds = np.asarray(train_embeds)\n",
    "temp = np.ones((train_embeds.shape[0], train_embeds.shape[1] + 1))\n",
    "temp[:,:-1] = train_embeds\n",
    "train_embeds = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "We will use one v/s all Logistic Regression as this is multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logisitic_regression:\n",
    "    \n",
    "    def __init__(self, tag, inp, out):\n",
    "        self.tag = tag\n",
    "        self.inp = inp\n",
    "        self.out = out\n",
    "        self.weights = np.zeros((train_embeds.shape[1], 1))\n",
    "        self.iterations = 300\n",
    "        self.lr = 0.99\n",
    "        \n",
    "        # Consider 1 for which tag model is being trained, others will be considered as 0\n",
    "        self.out = np.asarray([1 if i == self.tag else 0 for i in self.out]).reshape(-1,1)\n",
    "        \n",
    "        # train method will update weights.\n",
    "        self.train(self.inp, self.out, self.weights)\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, u):\n",
    "        return (1.0 / (1.0 + np.exp((-1.0) * u)))\n",
    "\n",
    "\n",
    "    def hypothesis(self, w, X):\n",
    "        return self.sigmoid(np.matmul(X, w))\n",
    "        \n",
    "        \n",
    "    def cost(self, X, Y, w):\n",
    "        h = self.hypothesis(w, X)\n",
    "        cost = (-1) * (np.matmul(Y.T, np.log(h)) + np.matmul(1 - Y.T, np.log(1 - h))) / X.shape[0]\n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, X, Y, w, lr, iterations):\n",
    "        loss_list = []\n",
    "        for i in range(iterations):\n",
    "            loss = self.cost(X, Y, w)\n",
    "            loss_list.append(loss)\n",
    "            gradient = np.matmul(X.T, (self.hypothesis(w, X) - Y)) / X.shape[0]\n",
    "            w = w - lr * gradient\n",
    "        return loss_list, w\n",
    "        \n",
    "        \n",
    "    def train(self, inp, out, w):\n",
    "        losses, trained_w = self.gradient_descent(inp, out, w, self.lr, self.iterations)\n",
    "        self.weights = trained_w\n",
    "        print(\"Trained for : \" + self.tag)\n",
    "        \n",
    "    # We will use this to get score of a given word after training.   \n",
    "    def get_score(self, X):\n",
    "        return self.hypothesis(self.weights, X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For each label we train seperate LR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for : ADP\n",
      "Trained for : PRT\n",
      "Trained for : NOUN\n",
      "Trained for : DET\n",
      "Trained for : X\n",
      "Trained for : ADV\n",
      "Trained for : VERB\n",
      "Trained for : PRON\n",
      "Trained for : NUM\n",
      "Trained for : CONJ\n",
      "Trained for : ADJ\n",
      "Trained for : .\n"
     ]
    }
   ],
   "source": [
    "possible_tags = set(train_tags)\n",
    "models = {}\n",
    "\n",
    "# We train models for all the possible labels we have seen in train data.\n",
    "# If there are N unique labels in train data, we will have N models to be trained.\n",
    "for t in possible_tags:\n",
    "    models[t] = logisitic_regression(t, train_embeds, train_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "We test the trained model on the data split we had done before and find the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set : 73.23 %\n"
     ]
    }
   ],
   "source": [
    "# Function returns a tag for a given word. It calculates score for each tag and returns tag with maximum score.\n",
    "def pred_tag(word):\n",
    "    scores = {}\n",
    "    for k, v in models.items():\n",
    "        scores[k] = v.get_score(np.asarray([np.append(get_embed(word), 1)]))\n",
    "    return sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[0][0]\n",
    "\n",
    "# Predict tags for each word\n",
    "pred_tags = [pred_tag(test_word) for test_word in test_words]\n",
    "# Find the accuracy of predicted tags\n",
    "accuracy_list = [1 if pred_tags[i] == test_tags[i] else 0 for i in range(len(pred_tags))]\n",
    "print('Accuracy on test set : ' + \"{0:.2f}\".format(sum(accuracy_list)/len(accuracy_list)*100) + ' %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence POS Prediction\n",
    "\n",
    "Predicting POS for a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mrs            VERB      \n",
      "Miller         NOUN      \n",
      "wants          VERB      \n",
      "the            DET       \n",
      "entire         DET       \n",
      "house          NOUN      \n",
      "repainted      VERB      \n",
      ".              .         \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def pred_tag_sequence(sentence):\n",
    "    for w in nltk.word_tokenize(sentence):\n",
    "        print('{:<15s}'.format(w) + '{:<10s}'.format(pred_tag(w)))\n",
    "\n",
    "pred_tag_sequence(\"Mrs Miller wants the entire house repainted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "We can trying predicting POS given some context as well, so by using bigram or higher n-gram. We can concatenate the embedding vectors or even try averaging embedding vectors of surrounding words(a window around the center word of whose POS we are trying to predict)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
